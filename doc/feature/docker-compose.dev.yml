services:
  # 1. 反向代理：Nginx (Alpine)
  nginx:
    image: nginx:alpine
    container_name: dev-petcare-nginx
    environment:
      - TZ=Asia/Shanghai
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/logs:/var/log/nginx
      - /Users/michael/VueProjects/pet-care-vue/dist:/usr/share/nginx/html # 指定项目静态页面打包路径
    ports:
      - "80:80"
      - "443:443"
    restart: unless-stopped
    networks:
      - petcare-network
    # Linux Docker 环境：添加 host.docker.internal 支持（映射到 Docker 网关）
    extra_hosts:
      - "host.docker.internal:host-gateway"
  # 2. 数据库：MySQL 5.7.40
  mysql:
    image: mysql:5.7.40
    container_name: dev-petcare-mysql
    environment:
      TZ: Asia/Shanghai    #时区
      LANG: en_US.UTF-8    #语言编码
      MYSQL_ROOT_PASSWORD: "!QAZ2wsx"
      MYSQL_DATABASE: mysql
      MYSQL_USER: petcareadmin
      MYSQL_PASSWORD: "!QAZ2wsx"
    volumes:
      - ./mysql/my.cnf:/etc/mysql/my.cnf
      - ./mysql/data:/var/lib/mysql
      - ./mysql/init-scripts:/docker-entrypoint-initdb.d/ # 初始化数据库脚本
      - ./mysql/tmp:/var/tmp # 挂载临时文件, 如需要导入dump，可进入容器，登录 MySQL，再使用 source /var/tmp/dump.sql导入
    ports:
      - "3306:3306"
    restart: unless-stopped
    networks:
      - petcare-network

  # 3. 缓存：Redis 6.2 Alpine
  redis:
    image: redis:6.2-alpine
    container_name: dev-petcare-redis
    volumes:
      - ./redis/redis.conf:/usr/local/etc/redis/redis.conf
      - ./redis/data:/data
    command: redis-server /usr/local/etc/redis/redis.conf
    ports:
      - "6379:6379"
    restart: unless-stopped
    networks:
      - petcare-network

  # 4.消息队列：kafka 7.7.7(KRaft模式)
  kafka:
    image: confluentinc/cp-kafka:7.7.7
    container_name: dev-petcare-kafka
    ports:
      - "9092:9092"
      - "9093:9093"
      - "9094:9094"
    environment:
      # KRaft 模式：集群ID，必需
      CLUSTER_ID: 7ihVPkZwTra52euGnMmPww
      # KRaft 模式：节点角色，broker(数据节点) 和 controller(控制节点)，单节点模式两者都需要
      # 可修改：多节点集群时可分离，如 "broker" 或 "controller"
      KAFKA_PROCESS_ROLES: broker,controller
      # 节点ID，集群中唯一标识
      # 可修改：多节点时改为 2,3,4... 等唯一值
      KAFKA_NODE_ID: 1
      # 控制器投票者列表，格式：nodeId@host:port（多节点时添加更多投票者，如 "1@kafka1:9094,2@kafka2:9094,3@kafka3:9094"）
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9094
      # 监听器配置：PLAINTEXT(外部客户端), CONTROLLER(控制器通信), PLAINTEXT_INTERNAL(内部broker通信)
      # 可修改：端口可改，但需与 ports 映射和 ADVERTISED_LISTENERS 保持一致
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9094,PLAINTEXT_INTERNAL://0.0.0.0:9093
      # 对外公布的监听器地址，客户端连接时使用 (可修改：localhost 可改为实际IP或域名，kafka 为容器内服务名)
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:9093
      # 监听器安全协议映射(生产环境，PLAINTEXT 建议改为 SSL/SASL)
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      # broker 间通信使用的监听器名称
      # 可修改：需与 LISTENERS 中的内部监听器名称一致
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
      # 控制器监听器名称
      # 可修改：需与 LISTENERS 中的控制器监听器名称一致
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      # offsets topic 副本数，单节点必须为 1
      # 可修改：多节点集群建议设为 3
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      # 事务状态日志副本数，单节点必须为 1
      # 可修改：多节点集群建议设为 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      # 事务状态日志最小同步副本数，单节点必须为 1(多节点集群建议设为 2)
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      # 自动创建 topic，开发环境建议开启(生产环境建议设为 "false"，手动管理 topic)
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      # 日志保留时间（小时），168=7天
      KAFKA_LOG_RETENTION_HOURS: 168
      # 日志段文件大小（字节），1073741824=1GB(根据磁盘和性能调整)
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      # 日志保留检查间隔（毫秒），300000=5分钟 (检查频率，如 60000(1分钟), 600000(10分钟))
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
    volumes:
      - ./kafka/data:/var/lib/kafka/data
    networks:
      - petcare-network

  # 4. 消息队列可视化：kafka-ui
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: dev-petcare-kafka-ui
    depends_on:
      - kafka
    ports:
      - "9090:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9093
    networks:
      - petcare-network

  # 5. 数据库：PostgreSQL 16.11
  pgvector:
    image: pgvector/pgvector:0.8.0-pg16
    container_name: dev-petcare-pgvector
    environment:
      POSTGRES_DB: pet_care_ai_vector
      POSTGRES_USER: postgres_root
      POSTGRES_PASSWORD: "!QAZ2wsx"
    # volumes:
    #   # 挂载数据卷以持久化数据
    #   - ./pgvector/data:/var/lib/postgresql/data/pgdata:ro
      # 可选：挂载初始化脚本目录
      # - ./pgvector/init-scripts:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    restart: unless-stopped
    networks:
      - petcare-network

  # 6. 搜索引擎：Elasticsearch 8.17.0
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.17.0
    container_name: dev-petcare-es
    privileged: true
    environment:
      #  重要：不实用SSL加密
      - xpack.security.enabled=false
      - xpack.security.http.ssl.enabled=false
      - xpack.security.transport.ssl.enabled=false
      # 设置集群名称
      - cluster.name=elasticsearch
      # 设置节点名称
      - node.name=es01
      # 启用单节点发现模式（开发环境使用）
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      # 开启 knn 向量检索增强插件
      - xpack.ml.enabled: true
    volumes:
      # 挂载数据目录，持久化存储ES数据
      - ./elasticsearch/data:/usr/share/elasticsearch/data
      - ./elasticsearch/plugins:/var/tmp
      # 挂载本地zip安装包到/var/tmp路径下，避免生成.DS_Store文件导致启动失败
      # 安装需要确认操作，进入容器执行本地安装命令 /usr/share/elasticsearch/bin/elasticsearch-plugin install file:///var/tmp/elasticsearch-analysis-ik-8.17.0.zip
    ports:
      - "9200:9200"
      - "9300:9300"
    restart: unless-stopped
    networks:
      - petcare-network
  # 6. 可视化：kibana 8.17.0
  kibana:
    image: docker.elastic.co/kibana/kibana:8.17.0
    container_name: dev-petcare-kibana
    environment:
      # 设置Elasticsearch实例地址
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=kibana
      - ELASTICSEARCH_PASSWORD=kibana
    ports:
      - "5601:5601"
    restart: unless-stopped
    depends_on:
      - elasticsearch
    networks:
      - petcare-network
  # 7. CDC（Change Data Capture，变更数据捕获）- Canal Server
  # 作用：监听 MySQL Binlog，将数据变更推送到 Kafka
  # 数据流：MySQL → Canal → Kafka → ES Consumer
  canal-server:
    image: canal/canal-server:v1.1.7
    container_name: canal-server
    ports:
      - "11111:11111"   # TCP 客户端连接端口（如需直连 Canal Client）
      - "11112:11112"   # 管理端口（metrics 查询）
    environment:
      # Instance 配置：core 对应 conf/core/instance.properties
      - canal.auto.scan=false              # 关闭自动扫描，手动指定 destinations
      - canal.destinations=core             # Instance 名称，对应 conf/core/ 目录

      # MySQL 连接配置（覆盖 instance.properties，可选）
      - canal.instance.master.address=mysql:3306
      - canal.instance.dbUsername=canal
      - canal.instance.dbPassword=canal

      # Kafka 配置（通过配置文件指定，此处保留用于文档说明）
      # 实际 topic 路由规则在 conf/core/instance.properties 的 canal.mq.dynamicTopic 中定义
      - canal.mq.servers=kafka:9092
    volumes:
      - ./conf/canal.properties:/home/admin/canal-server/conf/canal.properties
      - ./conf/core/instance.properties:/home/admin/canal-server/conf/core/instance.properties
      - ./canal/data:/home/admin/canal-server/data
    restart: unless-stopped
    depends_on:
      - mysql
      - kafka
    networks:
      - petcare-network

networks:
  petcare-network:
    driver: bridge